# -*- coding: utf-8 -*-
"""LDA_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T7zsRvqPM5hI6CM_cRB3kcJB3UosVQBj

**Copyright: Â© NexStream Technical Education, LLC**.  
All rights reserved

# LDA
The LDA class includes the following contructor and methods:


*   def __init__(self, num_dim_keep):
*   def compute_scatter(self, X, y):
*   def compute_lin_discriminants(self, X, y):
*   def project_axes(self, X):
UPDATE THIS LIST

<br>

Follow the procedure and complete the code in the cell below.

1.  Compute the scatter matrices  
  $S_w=\sum_{\text{i=1}}^{\text{n}} (N_i-1)S_i = \sum_{\text{i=1}}^{\text{n}}\sum_{\text{j=1}}^{\text{N}_i}(x_{i,j}-\bar x_i)(x_{i,j}-\bar x_i)^T$

  $S_b=\sum_{\text{i=1}}^{\text{c}} N_i(\bar x_i-\bar x)(\bar x_i-\bar x)^T$

2.  Perform eigenanalysis on scatter matrices:  Sw-1 Sb
3.  Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with largest eigenvalues to form projection matrix (i.e. the linear discriminants)
4.  Project data into new subspace

**Part 1:**   
From-scratch implementation
"""

from sklearn import datasets
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import numpy as np


class LDA:

  #### Class constructor
  # def __init__(self, num_dim_keep):
  #   inputs:  num_dim_keep = number of dimensions to keep
  #   instance variables:
  #     S_b:                  Between-class scatter
  #     S_w:                  Within-class scatter
  #     linear_discriminants: linear discriminants
  #     num_features:         Number of features in dataset
  #     num_classes:          Number of classes (categories) in dataset
  #     eigenvalues:          Eigenvalues of (S_w^-1)S_b
  #     eigenvectors:         Eigenvectors of (S_w^-1)S_b
  #     mean_overall:         Overall mean of dataset
  #     mean_diff_shape:      Modified shape of class mean - overall mean
  def __init__(self, num_dim_keep):
    self.num_dim_keep = num_dim_keep

    self.S_b = None                  # Between-class scatter
    self.S_w = None                  # Within-class scatter
    self.linear_discriminants = None # Linear discriminants
    self.num_features = None         # Number of features in dataset
    self.num_classes = None          # Number of classes in dataset
    self.eigenvalues = None          # Eigenvalues of (S_w^-1)S_b
    self.eigenvectors = None         # Eigenvectors of (S_w^-1)S_b
    self.mean_overall = None         # Overall mean of dataset
    self.mean_diff_shape = None      # Modified shape of class mean - overall mean
    #YOUR CODE HERE

  ####  Method compute_scatter(self, X, y):
  #  def compute_scatter(self, X, y):
  #  Compute between-class variance S_b, and within-class variance S_w
  #     S_w = sum over c classes (distance from each sample to class mean)
  #     S_b = sum over c classes (distance from each class mean to overall mean)
  #  inputs:
  #     X == matrix of features and examples
  #       X.shape[0] == number of examples (rows)
  #       X.shape[1] == number of features (cols)
  #  return:  Within-class and Between-class scatter matrices: S_w, S_b
  def compute_scatter(self, X, y):
    self.num_features = X.shape[1]
    self.num_classes = len(np.unique(y))
    self.mean_overall = np.mean(X, axis=0)
    self.S_w = np.zeros((self.num_features, self.num_features))  # Within-class scatter
    self.S_b = np.zeros((self.num_features, self.num_features))  # Between-class scatter
    for label in np.unique(y):
      class_data = X[y == label]
      class_mean = np.mean(class_data, axis=0)
      scatter_within = np.dot((class_data - class_mean).T, (class_data - class_mean))
      self.S_w += scatter_within
    for label in np.unique(y):
      class_data = X[y == label]
      class_mean = np.mean(class_data, axis=0)
      mean_diff = (class_mean - self.mean_overall).reshape(-1, 1)
      scatter_between = len(class_data) * np.dot(mean_diff, mean_diff.T)
      self.S_b += scatter_between

    return self.S_w, self.S_b
    #Update num_features with the number of features in X
    #YOUR CODE HERE


    #Initialize S_w and S_b to 0 for loop-based summation, with
    #matrix dimensions = d x d where d = number of features in dataset
    #YOUR CODE HERE
    self.S_w = np.zeros((self.num_features, self.num_features))
    self.S_b = np.zeros((self.num_features, self.num_features))


    #Compute S_w
    #loop over the number of classes in dataset
    #   calculate class mean
    #   for each X in class c, calculate:
    #     S_w = distance from each sample in c to class mean
    #     S_w += dot product[(X in c - class mean), transpose(X in c - class mean)]
    #       Note in implementation, we transpose the first term vs.
    #       what is shown in the formula. This is done to generate the correct
    #       shape for S_w.  We want S_w.shape to be dxd where
    #       d = number of features.  Since X = n x d
    #       (number of examples x number of features)
    #       then the shape of (nxd)dot(nxd).T = nxn (i.e.number of examples).
    #       Therefore, we need to reverse the order of transpose so that
    #       shape of ((nxd).T)dot(nxd) = dxd
    #YOUR CODE HERE
    num_features = X.shape[1]
    S_w = np.zeros((num_features, num_features))
    for label in np.unique(y):
      class_data = X[y == label]
      class_mean = np.mean(class_data, axis=0)
      diff = class_data - class_mean
      S_w += np.dot(diff.T, diff)


    #Compute S_b
    #calculate the overall mean of the dataset
    #loop over the number of classes in dataset
    #     calculate class mean
    #     S_b = sum over c classes (distance from each class mean to overall mean)
    #     S_b += number of sampls in class *
    #            dot product[(class mean - overall mean),
    #                         transpose(class mean - overall mean)]
    #       Note in implementation, we need to reshape (class mean - overall mean)
    #       prior to taking dot product, to generate the correct S_b shape.
    #       Since (class mean - overall mean) will have shape 1xd where
    #       d = number of features, we need to reshape to dx1.  Then the shape
    #       of S_b will be (dx1)dot(dx1).T = dxd
    #YOUR CODE HERE
    num_features = X.shape[1]
    overall_mean = np.mean(X, axis=0)
    S_b = np.zeros((num_features, num_features))
    for label in np.unique(y):
      class_data = X[y == label]
      class_mean = np.mean(class_data, axis=0)
      num_samples_in_class = class_data.shape[0]
      mean_diff = (class_mean - overall_mean).reshape(num_features, 1)
      S_b += num_samples_in_class * np.dot(mean_diff, mean_diff.T)



  ####  Method compute_lin_discriminants(self, X, y)
  #  def compute_lin_discriminants(self, X, y):
  #  inputs:
  #     X == matrix of features and examples
  #       X.shape[0] == number of examples (rows)
  #       X.shape[1] == number of features (cols)
  #     y == class labels
  #  outputs:  set self.linear_discrimants to top self.num_dim_keep eigenvectors
  #
  #  Maximize between-class variance, minimize within class variance
  #     Generate S_w, S_b:  self.compute_scatter()
  #     Perform eigenanalysis on (S_w^-1)(S_b)
  #     Order the eigenvectors, eigenvalues from high to low
  #     Select k linear discriminants to keep: self.num_dim_keep
  def compute_lin_discriminants(self, X, y):
    self.compute_scatter(X, y)
    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(self.S_w).dot(self.S_b))
    sorted_indices = np.argsort(eig_vals)[::-1]
    eig_vals = eig_vals[sorted_indices]
    eig_vecs = eig_vecs[:, sorted_indices]
    self.linear_discriminants = eig_vecs[:, :self.num_dim_keep]

    #Compute (S_w^-1)(S_b) and perform eigenanalysis
    #YOUR CODE HERE
    S_w_inv = np.linalg.inv(self.S_w)
    matrix = np.dot(S_w_inv, self.S_b)
    eig_vals, eig_vecs = np.linalg.eig(matrix)
    sorted_indices = np.argsort(eig_vals)[::-1]
    eig_vals = eig_vals[sorted_indices]
    eig_vecs = eig_vecs[:, sorted_indices]
    self.linear_discriminants = eig_vecs[:, :self.num_dim_keep]

    #Order the eigenvectors, eigenvalues from high to low
    #   Sort abs of eigenvalues in decreasing order, then store in
    #   eigenvectors and eigenvalues arrays (now in decreasing order).
    #Save self.linear_discriminants as eigenvectors from 0 to num_dim_keep
    #YOUR CODE HERE
    eig_vals, eig_vecs = np.linalg.eig(matrix)
    sorted_indices = np.argsort(np.abs(eig_vals))[::-1]
    eig_vals = eig_vals[sorted_indices]
    eig_vecs = eig_vecs[:, sorted_indices]
    self.linear_discriminants = eig_vecs[:, :self.num_dim_keep]

  #### Method project_axes(self, X)
  #  def project_axes(self, X):
  #  inputs:
  #     X == matrix of features and examples
  #       X.shape[0] == number of examples (rows)
  #       X.shape[1] == number of features (cols)
  #  return:  X projected onto the linear discriminant axes
  def project_axes(self, X):
    X_projected = np.dot(X, self.linear_discriminants)
    return X_projected

#Test data for Part 1

X = np.array([[2,3,4,5,6,6,5,8,9,11,11,12,13,14,8,8,10,10,12,13,13],
              [5,8,6,7,6,8,8,3,5,3,5,6,5,7,7,10,10,8,9,7,9]]).T
y = np.array([0,0,0,0,0,0,0,1,1,1,1,1,1,1,2,2,2,2,2,2,2]).T


label_colors = ['green', 'magenta', 'blue']
blue_patch = mpatches.Patch(color='blue', label='Waitlisted')
green_patch = mpatches.Patch(color='green', label='Rejected')
mag_patch = mpatches.Patch(color='magenta', label='Accepted')


lda = LDA(2)
lda.compute_scatter(X, y)
lda.compute_lin_discriminants(X, y)
X_projected = lda.project_axes(X)
print('Shape of X:', X.shape)
print('Shape of transformed X:', X_projected.shape)
print('Projected data', X_projected)


fig, ax = plt.subplots(1, 2, figsize=(10, 4))
fig.colorbar
ax[0].scatter(X[:,0], X[:,1],
            c=y, edgecolor='none', alpha=0.8,
            cmap=matplotlib.colors.ListedColormap(label_colors))
ax[0].legend(loc=2, prop={'size': 8}, handles=[mag_patch, green_patch, blue_patch])
ax[0].set_xlabel('Avg Sleep')
ax[0].set_ylabel('Num AP Classes Taken')

ax[1].scatter(X_projected[:,0], X_projected[:,1],
            c=y, edgecolor='none', alpha=0.8,
            cmap=matplotlib.colors.ListedColormap(label_colors))
ax[1].legend(loc=2, prop={'size': 8}, handles=[mag_patch, green_patch, blue_patch])
ax[1].set_xlabel('Linear Discriminant 1')
ax[1].set_ylabel('Linear Discriminant 2')


#Run the doctest module.  DO NOT modify any code below this line!
import doctest
"""
  >>> print(lda.num_features)
  2
  >>> print(lda.num_classes)
  [0 1 2]
  >>> print(np.around(lda.eigenvalues, 3))
  [3.845 1.258]
  >>> print(np.around(lda.eigenvectors, 3))
  [[ 0.717 -0.697]
   [ 0.275  0.961]]
  >>> print(np.around(lda.mean_overall, 3))
  [8.714 6.762]
  >>> print(np.around(lda.S_w, 3))
  [[68.286 15.286]
   [15.286 31.429]]
  >>> print(np.around(lda.S_b, 3))
  [[194.    -11.714]
   [-11.714  48.381]]
"""
doctest.testmod(verbose=True)

"""**Part 2:**
Test on Iris dataset
"""

#Test data from iris


data = datasets.load_iris()
X = data.data
y = data.target
lda = LDA(2)
lda.compute_scatter(X, y)
lda.compute_lin_discriminants(X, y)
X_projected = lda.project_axes(X)
print('Shape of X:', X.shape)
print('Shape of transformed X:', X_projected.shape)

x1 = X_projected[:, 0]
x2 = X_projected[:, 1]

plt.scatter(x1, x2,
            c=y, edgecolor='none', alpha=0.8,
            cmap=plt.cm.get_cmap('viridis', 3))
plt.colorbar()
plt.xlabel('Linear Discriminant 1')
plt.ylabel('Linear Discriminant 2')
plt.show()


#Run the doctest module.  DO NOT modify any code below this line!
import doctest
"""
  >>> print(lda.num_features)
  4
  >>> print(lda.num_classes)
  3
  >>> print(np.around(lda.eigenvalues, 3))
  [32.192  0.285 -0.    -0.   ]
  >>> print(np.around(lda.eigenvectors, 3))
  [[-0.209 -0.386  0.554  0.707]
   [-0.007 -0.587  0.253 -0.769]
   [ 0.152  0.282  0.355 -0.878]
   [ 0.879 -0.328 -0.314 -0.144]]
  >>> print(np.around(lda.mean_overall, 3))
  [5.843 3.057 3.758 1.199]
  >>> print(np.around(lda.S_w, 3))
  [[38.956 13.63  24.625  5.645]
   [13.63  16.962  8.121  4.808]
   [24.625  8.121 27.223  6.272]
   [ 5.645  4.808  6.272  6.157]]
  >>> print(np.around(lda.S_b, 3))
  [[ 63.212 -19.953 165.248  71.279]
   [-19.953  11.345 -57.24  -22.933]
   [165.248 -57.24  437.103 186.774]
   [ 71.279 -22.933 186.774  80.413]]
"""
doctest.testmod(verbose=True)